{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:00.306140Z",
     "start_time": "2020-01-22T11:10:59.053540Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n",
      "1.3.1\n",
      "GeForce RTX 2070 SUPER\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# In[1]:\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "import glob\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import gc\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import json\n",
    "from typing import NamedTuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "print(torch.__version__)\n",
    "# from tools import eval_summary, save_feature_importance, merge_preds\n",
    "from tools import EarlyStopping\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:00.309090Z",
     "start_time": "2020-01-22T11:11:00.306987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[2]:\n",
    "torch.set_num_threads(8)\n",
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:11.897538Z",
     "start_time": "2020-01-22T11:11:00.309932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(810000, 230) (10000, 227)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('input/train.csv', dtype=np.float32)\n",
    "df_test = pd.read_csv('input/test.csv', dtype=np.float32)\n",
    "print(df_train.shape, df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:11.901321Z",
     "start_time": "2020-01-22T11:11:11.898519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_cols = [c for c in df_train.columns if 'layer_' in c]\n",
    "fea_cols = [c for c in df_train.columns if c not in layer_cols]\n",
    "\n",
    "len(fea_cols), len(layer_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:11.914006Z",
     "start_time": "2020-01-22T11:11:11.902046Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DNN1Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:11.919388Z",
     "start_time": "2020-01-22T11:11:11.915161Z"
    }
   },
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('input/SiO2.txt', sep='\\t')\n",
    "# df2 = pd.read_csv('input/Si3N4.txt', sep='\\t')\n",
    "\n",
    "# df_nk = pd.merge(df1, df2, on='Wavelength(nm)')\n",
    "# df_nk = df_nk[:226]\n",
    "\n",
    "# df_nk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:11.975181Z",
     "start_time": "2020-01-22T11:11:11.920538Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNN1Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, dropout_probability=0.3):\n",
    "        super().__init__()\n",
    "        relu = torch.nn.ReLU()\n",
    "        dropout = torch.nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        self.layer_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "        )\n",
    "        self.layer_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "        )\n",
    "        self.layer_3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "        )\n",
    "        self.layer_4 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "            torch.nn.Linear(input_size, input_size), relu, torch.nn.BatchNorm1d(input_size), dropout, \n",
    "        )\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, input_size), relu,\n",
    "            torch.nn.Linear(input_size, input_size), relu,\n",
    "            torch.nn.Linear(input_size, input_size), relu,\n",
    "            torch.nn.Linear(input_size, 4),\n",
    "        )\n",
    "        \n",
    "        self.layer13_n = torch.Tensor(df_nk[['n_x']].T.values).to(device)\n",
    "        self.layer13_k = torch.Tensor(df_nk[['k_x']].T.values).to(device)\n",
    "        self.layer24_n = torch.Tensor(df_nk[['n_y']].T.values).to(device)\n",
    "        self.layer24_k = torch.Tensor(df_nk[['k_y']].T.values).to(device)\n",
    "        \n",
    "    \n",
    "    def forward(self, x_fea):\n",
    "        \n",
    "        out_layer_1 = self.layer_1(torch.add(torch.mul(x_fea, self.layer13_n), self.layer13_k))\n",
    "        out_layer_2 = self.layer_2(torch.add(torch.mul(out_layer_1, self.layer24_n), self.layer24_k))\n",
    "        out_layer_3 = self.layer_3(torch.add(torch.mul(out_layer_2, self.layer13_n), self.layer13_k))\n",
    "        out_layer_4 = self.layer_4(torch.add(torch.mul(out_layer_2, self.layer24_n), self.layer24_k))\n",
    "        \n",
    "        return self.fc(out_layer_4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:11.982358Z",
     "start_time": "2020-01-22T11:11:11.976373Z"
    }
   },
   "outputs": [],
   "source": [
    "# # size = 48\n",
    "# W = 15 # input_volume_size\n",
    "# F = 6  # kernel_size\n",
    "# S = 1   # strides\n",
    "# P = 1\n",
    "# # padding_size\n",
    "\n",
    "# size = (W - F + 2*P) / S + 1\n",
    "# size\n",
    "# # ((size - 1) * S) - 2*P + F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:11.992318Z",
     "start_time": "2020-01-22T11:11:11.983206Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNModel(torch.nn.Module):\n",
    "    def __init__(self, dropout_probability=0.3):\n",
    "        super().__init__()\n",
    "        relu = torch.nn.ReLU()\n",
    "        dropout = torch.nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1, 2, 31, stride=1, padding=0), #196\n",
    "            relu, torch.nn.MaxPool1d(2), #98 \n",
    "            torch.nn.Conv1d(2, 4, 19, stride=1, padding=0), #80\n",
    "            relu, torch.nn.MaxPool1d(2), #40\n",
    "            torch.nn.Conv1d(4, 8, 11, stride=1, padding=0), #30\n",
    "            relu, torch.nn.MaxPool1d(2), #15\n",
    "            torch.nn.Conv1d(8, 16, 6, stride=1, padding=1), #12\n",
    "            relu, torch.nn.MaxPool1d(2), #6\n",
    "        )\n",
    "            \n",
    "# #             torch.nn.Linear(input_size, 4),\n",
    "#             torch.nn.Linear(input_size, 200), relu, #torch.nn.BatchNorm1d(200), dropout, \n",
    "#             torch.nn.Linear(200, 200), relu, #torch.nn.BatchNorm1d(200), dropout,\n",
    "#             torch.nn.Linear(200, 200), relu, #torch.nn.BatchNorm1d(200), dropout,\n",
    "#             torch.nn.Linear(200, 150), relu, #torch.nn.BatchNorm1d(200), dropout,\n",
    "#             torch.nn.Linear(150, 128), relu, #torch.nn.BatchNorm1d(128), dropout,\n",
    "#             torch.nn.Linear(128, 128), relu, #torch.nn.BatchNorm1d(128), dropout,\n",
    "#             torch.nn.Linear(128, 100), relu, #torch.nn.BatchNorm1d(128), dropout,\n",
    "#             torch.nn.Linear(100, 64), relu, #torch.nn.BatchNorm1d(128), dropout,\n",
    "#             torch.nn.Linear(64, 32), relu, #torch.nn.BatchNorm1d(128), dropout,\n",
    "#             torch.nn.Linear(32, 16), relu, #torch.nn.BatchNorm1d(128), dropout,\n",
    "#             torch.nn.Linear(16, 8), relu, #torch.nn.BatchNorm1d(128), dropout,\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16*6, 64), relu, #torch.nn.BatchNorm1d(128), dropout,\n",
    "            torch.nn.Linear(64, 4)\n",
    "       )\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        out = self.cnn(x)\n",
    "        dim = 1\n",
    "        for d in out.size()[1:]: #24, 4, 4\n",
    "            dim = dim * d\n",
    "        out = out.view(-1, dim)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "        return self.model(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:11.999880Z",
     "start_time": "2020-01-22T11:11:11.993149Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, dropout_probability=0.3):\n",
    "        super(DNNModel,self).__init__()\n",
    "#         relu = torch.nn.ReLU()\n",
    "        act = torch.nn.ELU()\n",
    "        dropout = torch.nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 2048), torch.nn.BatchNorm1d(2048), act, dropout, \n",
    "            torch.nn.Linear(2048, 2048), torch.nn.BatchNorm1d(2048), act, dropout, \n",
    "            torch.nn.Linear(2048, 1024), torch.nn.BatchNorm1d(1024), act, dropout,\n",
    "            torch.nn.Linear(1024, 1024), torch.nn.BatchNorm1d(1024), act, dropout,            \n",
    "            torch.nn.Linear(1024, 512), torch.nn.BatchNorm1d(512), act, dropout,\n",
    "            torch.nn.Linear(512, 512), torch.nn.BatchNorm1d(512), act, dropout,\n",
    "            torch.nn.Linear(512, 256), torch.nn.BatchNorm1d(256), act, dropout,            \n",
    "            torch.nn.Linear(256, 4)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:12.007623Z",
     "start_time": "2020-01-22T11:11:12.000659Z"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "class SemiDataset(Dataset):\n",
    "    def __init__(self, df, fea_cols, y_cols):        \n",
    "        self.X = df[fea_cols].values\n",
    "        self.y = df[y_cols].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].astype(np.float32), self.y[idx].astype(np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:12.017628Z",
     "start_time": "2020-01-22T11:11:12.008615Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, device):\n",
    "        self.device = device\n",
    "        self.model = model#.to(self.device)\n",
    "        self.criterion = criterion#.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        print(self.model.train())\n",
    "        pass\n",
    "    \n",
    "    def train(self, data_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for data in data_loader:\n",
    "            X_batch, y_batch = data\n",
    "            X_batch = X_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            y_pred = self.model(X_batch)\n",
    "#             print(y_pred, y_batch)\n",
    "            \n",
    "            loss = self.criterion(y_pred, y_batch)\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def eval(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "#         print('valid_loader', len(valid_loader))\n",
    "        for data in data_loader:\n",
    "            X_batch, y_batch = data\n",
    "            X_batch = X_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                total_loss = total_loss + loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def save(self, model_path='checkpoint.pt'):\n",
    "        torch.save(self.model.state_dict(), 'checkpoint.pt')\n",
    "        return\n",
    "    \n",
    "    def load(self, model_path='checkpoint.pt'):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T00:57:16.154252Z",
     "start_time": "2020-01-14T00:57:16.152105Z"
    }
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:12.025601Z",
     "start_time": "2020-01-22T11:11:12.018372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200122T201112\n",
      "fea_size 226 layer_cols ['layer_1', 'layer_2', 'layer_3', 'layer_4']\n"
     ]
    }
   ],
   "source": [
    "model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "print(model_ts)\n",
    "\n",
    "print(f'fea_size {len(fea_cols)} layer_cols {layer_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:12.031612Z",
     "start_time": "2020-01-22T11:11:12.026749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81511991154"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(81511991154)\n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:12.262936Z",
     "start_time": "2020-01-22T11:11:12.032761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700000 110000\n",
      "batch_size 50000 num_workers 4\n",
      "train_loader 14 val_loader 3\n"
     ]
    }
   ],
   "source": [
    "dataset = SemiDataset(df_model[fea_cols + layer_cols], fea_cols, layer_cols)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [700000, 110000])\n",
    "\n",
    "print(len(train_set), len(val_set))\n",
    "\n",
    "batch_size = 50000\n",
    "num_workers = 4\n",
    "\n",
    "loader_params = {    \n",
    "    'batch_size' : batch_size,\n",
    "    'shuffle' : True,\n",
    "    'num_workers' : num_workers,\n",
    "    'drop_last' : False,\n",
    "}\n",
    "train_loader = DataLoader(dataset=train_set, **loader_params)\n",
    "val_loader = DataLoader(dataset=val_set, **loader_params)\n",
    "\n",
    "print(f'batch_size {batch_size} num_workers {num_workers}')\n",
    "print(f'train_loader {len(train_loader)} val_loader {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:11:13.719127Z",
     "start_time": "2020-01-22T11:11:12.263796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNNModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=226, out_features=2048, bias=True)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ELU(alpha=1.0)\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ELU(alpha=1.0)\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (13): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ELU(alpha=1.0)\n",
      "    (15): Dropout(p=0.1, inplace=False)\n",
      "    (16): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ELU(alpha=1.0)\n",
      "    (19): Dropout(p=0.1, inplace=False)\n",
      "    (20): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (21): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ELU(alpha=1.0)\n",
      "    (23): Dropout(p=0.1, inplace=False)\n",
      "    (24): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (25): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ELU(alpha=1.0)\n",
      "    (27): Dropout(p=0.1, inplace=False)\n",
      "    (28): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = DNN1Model(input_size=len(fea_cols), dropout_probability=0.3).to(device)\n",
    "model = DNNModel(input_size=len(fea_cols), dropout_probability=0.1).to(device)\n",
    "#     model = CNNModel(dropout_probability=0.1).to(device)\n",
    "# model = joblib.load('checkpoint.model').to(device)\n",
    "\n",
    "    \n",
    "criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "# criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=300, gamma=0.9)\n",
    "\n",
    "\n",
    "trainer = Trainer(model, criterion, optimizer, scheduler, device)\n",
    "# trainer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:17:26.274519Z",
     "start_time": "2020-01-22T11:11:13.719966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7471e7f0305f4dab9ca974d1e3ff4968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2000, style=ProgressStyle(description_width='initâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20200122T201122] Epock 0 / 2000 train_loss: 150.56396702357702  valid_loss: 87.64200337727864\n",
      "Validation loss decreased (inf --> 87.64200338).  Saving model ...\n",
      "Current learning rate is: 0.01\n",
      "[20200122T201130] Epock 1 / 2000 train_loss: 133.2375204903739  valid_loss: 142.87035115559897\n",
      "EarlyStopping 2 / 100 counter: 1 out of 50\n",
      "[20200122T201138] Epock 2 / 2000 train_loss: 104.80827331542969  valid_loss: 103.3559799194336\n",
      "EarlyStopping 3 / 100 counter: 2 out of 50\n",
      "[20200122T201146] Epock 3 / 2000 train_loss: 74.40599332536969  valid_loss: 71.6258036295573\n",
      "Validation loss decreased (87.64200338 --> 71.62580363).  Saving model ...\n",
      "[20200122T201154] Epock 4 / 2000 train_loss: 63.18517630440848  valid_loss: 73.10904184977214\n",
      "EarlyStopping 5 / 100 counter: 1 out of 50\n",
      "[20200122T201202] Epock 5 / 2000 train_loss: 61.304196221487864  valid_loss: 64.25983174641927\n",
      "Validation loss decreased (71.62580363 --> 64.25983175).  Saving model ...\n",
      "[20200122T201210] Epock 6 / 2000 train_loss: 59.267986570085796  valid_loss: 58.90155792236328\n",
      "Validation loss decreased (64.25983175 --> 58.90155792).  Saving model ...\n",
      "[20200122T201218] Epock 7 / 2000 train_loss: 56.1679379599435  valid_loss: 61.20852915445963\n",
      "EarlyStopping 8 / 100 counter: 1 out of 50\n",
      "[20200122T201226] Epock 8 / 2000 train_loss: 53.73005049569266  valid_loss: 54.96502812703451\n",
      "Validation loss decreased (58.90155792 --> 54.96502813).  Saving model ...\n",
      "[20200122T201235] Epock 9 / 2000 train_loss: 52.32974542890276  valid_loss: 56.42151387532552\n",
      "EarlyStopping 10 / 100 counter: 1 out of 50\n",
      "[20200122T201243] Epock 10 / 2000 train_loss: 51.3753730228969  valid_loss: 54.18805058797201\n",
      "Validation loss decreased (54.96502813 --> 54.18805059).  Saving model ...\n",
      "[20200122T201251] Epock 11 / 2000 train_loss: 50.50714029584612  valid_loss: 53.692386627197266\n",
      "Validation loss decreased (54.18805059 --> 53.69238663).  Saving model ...\n",
      "[20200122T201259] Epock 12 / 2000 train_loss: 49.879956654139924  valid_loss: 51.2894032796224\n",
      "Validation loss decreased (53.69238663 --> 51.28940328).  Saving model ...\n",
      "[20200122T201307] Epock 13 / 2000 train_loss: 49.30531474522182  valid_loss: 50.521156311035156\n",
      "Validation loss decreased (51.28940328 --> 50.52115631).  Saving model ...\n",
      "[20200122T201315] Epock 14 / 2000 train_loss: 48.858944756644114  valid_loss: 47.97945785522461\n",
      "Validation loss decreased (50.52115631 --> 47.97945786).  Saving model ...\n",
      "[20200122T201324] Epock 15 / 2000 train_loss: 48.191093172345845  valid_loss: 47.2988026936849\n",
      "Validation loss decreased (47.97945786 --> 47.29880269).  Saving model ...\n",
      "[20200122T201332] Epock 16 / 2000 train_loss: 47.634062903267996  valid_loss: 46.58883412679037\n",
      "Validation loss decreased (47.29880269 --> 46.58883413).  Saving model ...\n",
      "[20200122T201340] Epock 17 / 2000 train_loss: 46.17618451799665  valid_loss: 44.54162470499674\n",
      "Validation loss decreased (46.58883413 --> 44.54162470).  Saving model ...\n",
      "[20200122T201348] Epock 18 / 2000 train_loss: 44.317957741873606  valid_loss: 42.98246765136719\n",
      "Validation loss decreased (44.54162470 --> 42.98246765).  Saving model ...\n",
      "[20200122T201356] Epock 19 / 2000 train_loss: 41.96735681806292  valid_loss: 40.81397501627604\n",
      "Validation loss decreased (42.98246765 --> 40.81397502).  Saving model ...\n",
      "[20200122T201404] Epock 20 / 2000 train_loss: 39.987985610961914  valid_loss: 37.94433848063151\n",
      "Validation loss decreased (40.81397502 --> 37.94433848).  Saving model ...\n",
      "[20200122T201413] Epock 21 / 2000 train_loss: 38.21388380868094  valid_loss: 36.17402267456055\n",
      "Validation loss decreased (37.94433848 --> 36.17402267).  Saving model ...\n",
      "[20200122T201421] Epock 22 / 2000 train_loss: 37.015232631138396  valid_loss: 34.921295166015625\n",
      "Validation loss decreased (36.17402267 --> 34.92129517).  Saving model ...\n",
      "[20200122T201429] Epock 23 / 2000 train_loss: 36.07286398751395  valid_loss: 33.59512710571289\n",
      "Validation loss decreased (34.92129517 --> 33.59512711).  Saving model ...\n",
      "[20200122T201437] Epock 24 / 2000 train_loss: 35.061426980154856  valid_loss: 32.69348271687826\n",
      "Validation loss decreased (33.59512711 --> 32.69348272).  Saving model ...\n",
      "[20200122T201445] Epock 25 / 2000 train_loss: 33.82685797555106  valid_loss: 31.25514030456543\n",
      "Validation loss decreased (32.69348272 --> 31.25514030).  Saving model ...\n",
      "[20200122T201454] Epock 26 / 2000 train_loss: 30.830756732395717  valid_loss: 27.007489522298176\n",
      "Validation loss decreased (31.25514030 --> 27.00748952).  Saving model ...\n",
      "[20200122T201502] Epock 27 / 2000 train_loss: 27.76523235866002  valid_loss: 24.194023768107098\n",
      "Validation loss decreased (27.00748952 --> 24.19402377).  Saving model ...\n",
      "[20200122T201510] Epock 28 / 2000 train_loss: 25.14808314187186  valid_loss: 20.051087697347004\n",
      "Validation loss decreased (24.19402377 --> 20.05108770).  Saving model ...\n",
      "[20200122T201518] Epock 29 / 2000 train_loss: 23.146709033421107  valid_loss: 18.974544525146484\n",
      "Validation loss decreased (20.05108770 --> 18.97454453).  Saving model ...\n",
      "[20200122T201526] Epock 30 / 2000 train_loss: 21.857849529811315  valid_loss: 17.407677968343098\n",
      "Validation loss decreased (18.97454453 --> 17.40767797).  Saving model ...\n",
      "[20200122T201534] Epock 31 / 2000 train_loss: 20.659434182303293  valid_loss: 15.962692896525065\n",
      "Validation loss decreased (17.40767797 --> 15.96269290).  Saving model ...\n",
      "[20200122T201542] Epock 32 / 2000 train_loss: 19.658582414899552  valid_loss: 14.987709681193033\n",
      "Validation loss decreased (15.96269290 --> 14.98770968).  Saving model ...\n",
      "[20200122T201551] Epock 33 / 2000 train_loss: 18.89357594081334  valid_loss: 14.700531323750814\n",
      "Validation loss decreased (14.98770968 --> 14.70053132).  Saving model ...\n",
      "[20200122T201559] Epock 34 / 2000 train_loss: 18.150376319885254  valid_loss: 12.928407033284506\n",
      "Validation loss decreased (14.70053132 --> 12.92840703).  Saving model ...\n",
      "[20200122T201607] Epock 35 / 2000 train_loss: 17.29101766858782  valid_loss: 12.108569145202637\n",
      "Validation loss decreased (12.92840703 --> 12.10856915).  Saving model ...\n",
      "[20200122T201615] Epock 36 / 2000 train_loss: 16.650714874267578  valid_loss: 11.796417554219564\n",
      "Validation loss decreased (12.10856915 --> 11.79641755).  Saving model ...\n",
      "[20200122T201623] Epock 37 / 2000 train_loss: 16.213069370814733  valid_loss: 10.910805066426596\n",
      "Validation loss decreased (11.79641755 --> 10.91080507).  Saving model ...\n",
      "[20200122T201632] Epock 38 / 2000 train_loss: 15.383188724517822  valid_loss: 9.742807706197103\n",
      "Validation loss decreased (10.91080507 --> 9.74280771).  Saving model ...\n",
      "[20200122T201640] Epock 39 / 2000 train_loss: 14.963840075901576  valid_loss: 9.984403610229492\n",
      "EarlyStopping 40 / 100 counter: 1 out of 50\n",
      "[20200122T201648] Epock 40 / 2000 train_loss: 14.49310575212751  valid_loss: 8.84551207224528\n",
      "Validation loss decreased (9.74280771 --> 8.84551207).  Saving model ...\n",
      "[20200122T201656] Epock 41 / 2000 train_loss: 13.91593599319458  valid_loss: 8.201767921447754\n",
      "Validation loss decreased (8.84551207 --> 8.20176792).  Saving model ...\n",
      "[20200122T201704] Epock 42 / 2000 train_loss: 13.49485274723598  valid_loss: 7.812312602996826\n",
      "Validation loss decreased (8.20176792 --> 7.81231260).  Saving model ...\n",
      "[20200122T201712] Epock 43 / 2000 train_loss: 13.050905772617885  valid_loss: 7.3802103996276855\n",
      "Validation loss decreased (7.81231260 --> 7.38021040).  Saving model ...\n",
      "[20200122T201721] Epock 44 / 2000 train_loss: 12.761641025543213  valid_loss: 7.253512382507324\n",
      "Validation loss decreased (7.38021040 --> 7.25351238).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/aiden/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/aiden/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/aiden/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/aiden/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aiden/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/aiden/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/aiden/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/aiden/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6f35f68234a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-11bb5eae8f3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#             print(y_pred, y_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2179\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=50, min_epoch=100, verbose=True)\n",
    "\n",
    "total_epoch = 2000\n",
    "\n",
    "for e in tqdm_notebook(range(total_epoch), total=total_epoch, desc='Epoch'):\n",
    "    if os.path.isfile('stop.flag'):\n",
    "        print(f'{e} stop!')\n",
    "        break\n",
    "\n",
    "    train_loss = trainer.train(train_loader)\n",
    "    valid_loss = trainer.eval(val_loader)\n",
    "    \n",
    "    ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "    print(f'[{ts}] Epock {e} / {total_epoch} train_loss: {train_loss}  valid_loss: {valid_loss}')\n",
    "    \n",
    "    early_stopping(valid_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"\\tEarly stopping epoch {}, valid loss {}\".format(e, valid_loss))\n",
    "        \n",
    "    if e % 100 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"Current learning rate is: {}\".format(param_group['lr']))\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "torch.save(model.state_dict(), 'model/{}_{}.model'.format(model_ts, early_stopping.best_epoch))\n",
    "    \n",
    "# torch.save(model.state_dict(), f'checkpoint.pt.{train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:17:26.275389Z",
     "start_time": "2020-01-22T11:10:59.134Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = joblib.load('checkpoint.model')\n",
    "model.eval()\n",
    "y_pred = model(torch.Tensor(df_test[fea_cols].values).to(device))    \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:17:26.275883Z",
     "start_time": "2020-01-22T11:10:59.136Z"
    }
   },
   "outputs": [],
   "source": [
    "df_submit = pd.read_csv('input/sample_submission.csv', index_col=0)\n",
    "df_submit[layer_cols] = y_pred.cpu().detach().numpy()\n",
    "ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "df_submit.to_csv(f'submit/{ts}_{valid_loss}.csv')\n",
    "print(ts, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "166px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
