{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:25.995515Z",
     "start_time": "2020-02-01T13:11:24.810081Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# In[1]:\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "import glob\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import gc\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import json\n",
    "from typing import NamedTuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "print(torch.__version__)\n",
    "# from tools import eval_summary, save_feature_importance, merge_preds\n",
    "from tools import EarlyStopping\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:26.000075Z",
     "start_time": "2020-02-01T13:11:25.996977Z"
    }
   },
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "torch.set_num_threads(8)\n",
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.443291Z",
     "start_time": "2020-02-01T13:11:26.001597Z"
    }
   },
   "outputs": [],
   "source": [
    "class SemiDataset(Dataset):\n",
    "    def __init__(self, df, fea_cols, y_cols):        \n",
    "        self.X = df[fea_cols].values\n",
    "        self.y = df[y_cols].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].astype(np.float32), self.y[idx].astype(np.float32)\n",
    "    \n",
    "\n",
    "df_train = pd.read_csv('input/train.csv', dtype=np.float32)\n",
    "df_test = pd.read_csv('input/test.csv', dtype=np.float32)\n",
    "\n",
    "print(df_train.shape, df_test.shape)\n",
    "\n",
    "layer_cols = [c for c in df_train.columns if 'layer_' in c]\n",
    "fea_cols = [c for c in df_train.columns if c not in layer_cols]\n",
    "\n",
    "len(fea_cols), len(layer_cols)\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(df_train[fea_cols].values)\n",
    "\n",
    "scaler = joblib.load('scaler.bin')\n",
    "\n",
    "df_train[fea_cols] = scaler.transform(df_train[fea_cols].values)\n",
    "df_test[fea_cols] = scaler.transform(df_test[fea_cols].values)\n",
    "\n",
    "torch.manual_seed(81511991154)\n",
    "torch.initial_seed()\n",
    "\n",
    "df_model = df_train\n",
    "\n",
    "dataset = SemiDataset(df_model[fea_cols + layer_cols], fea_cols, layer_cols)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [700000, 110000])\n",
    "\n",
    "print(len(train_set), len(val_set))\n",
    "\n",
    "batch_size = 25000\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, num_workers=num_workers)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=22000)\n",
    "\n",
    "print(f'batch_size {batch_size} num_workers {num_workers}')\n",
    "print(f'train_loader {len(train_loader)} val_loader {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.505829Z",
     "start_time": "2020-02-01T13:11:38.444324Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, device):\n",
    "        self.device = device\n",
    "        self.model = model#.to(self.device)\n",
    "        self.criterion = criterion#.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        print(self.model.train())\n",
    "        pass\n",
    "    \n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def set_scheduler(self, scheduler):\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def train_(self, X_batch, y_batch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        X_batch = X_batch.to(self.device)\n",
    "        y_batch = y_batch.to(self.device)\n",
    "\n",
    "        y_pred = self.model(X_batch)\n",
    "#             print(y_pred, y_batch)\n",
    "\n",
    "        loss = self.criterion(y_pred, y_batch)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def train(self, data_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for data in data_loader:\n",
    "            X_batch, y_batch = data\n",
    "            X_batch = X_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            y_pred = self.model(X_batch)\n",
    "#             print(y_pred, y_batch)\n",
    "            \n",
    "            loss = self.criterion(y_pred, y_batch)\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def eval(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "#         print('valid_loader', len(valid_loader))\n",
    "        for data in data_loader:\n",
    "            X_batch, y_batch = data\n",
    "            X_batch = X_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                total_loss = total_loss + loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def save(self, model_path='checkpoint.pt'):\n",
    "        joblib.dump(self.model, model_path)\n",
    "        return\n",
    "    \n",
    "    def load(self, model_path='checkpoint.pt'):\n",
    "        self.model = joblib.load(model_path)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.523676Z",
     "start_time": "2020-02-01T13:11:38.506571Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, dropout_probability=0.3):\n",
    "        super(DNNModel,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.531451Z",
     "start_time": "2020-02-01T13:11:38.524303Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# models = [\n",
    "#     'model/20200125T200128_0.4469637870788574.model',\n",
    "#     'model/20200124T233212_0.4282313088575999.model',\n",
    "#     'model/20200125T200128_0.4469637870788574.model',\n",
    "#     'model/20200129T232141_0.4588648915290833.model',\n",
    "#     'model/20200130T082430_0.4571056246757507.model',\n",
    "#     'model/20200130T085842_0.47272029519081116.model',\n",
    "#     'model/20200130T140408_0.4705016195774078.model',\n",
    "#     'model/20200125T200128_0.3261864085992177.model',    \n",
    "#     'model/20200130T140408_0.3449194014072418.model',\n",
    "#     'model/20200131T111430_0.4638769030570984.model',\n",
    "# ]\n",
    "\n",
    "# train_preds_list = []\n",
    "# valid_preds_list = []\n",
    "# test_preds_list = []\n",
    "# for i, model_path in enumerate(models):\n",
    "#     model = joblib.load(model_path)\n",
    "#     print(model_path)\n",
    "# #     print(model)\n",
    "#     criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "    \n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     preds = []\n",
    "#     train_y_true = []\n",
    "#     for data in train_loader:\n",
    "#         X_batch, y_batch = data\n",
    "#         X_batch = X_batch.to(device)\n",
    "#         y_batch = y_batch.to(device)\n",
    "#         train_y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             y_pred = model(X_batch)\n",
    "#             preds.extend(y_pred.cpu().numpy())\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             total_loss = total_loss + loss.item()\n",
    "\n",
    "#     train_loss_min = total_loss / len(train_loader)\n",
    "#     print('train_loss_min', train_loss_min)\n",
    "#     preds = np.stack(preds)\n",
    "#     train_preds_list.append(preds)\n",
    "    \n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     preds = []\n",
    "#     valid_y_true = []\n",
    "#     for data in val_loader:\n",
    "#         X_batch, y_batch = data\n",
    "#         X_batch = X_batch.to(device)\n",
    "#         y_batch = y_batch.to(device)\n",
    "#         valid_y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             y_pred = model(X_batch)\n",
    "#             preds.extend(y_pred.cpu().numpy())\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             total_loss = total_loss + loss.item()\n",
    "\n",
    "#     val_loss_min = total_loss / len(val_loader)\n",
    "#     print('val_loss_min', val_loss_min)\n",
    "#     preds = np.stack(preds)\n",
    "#     valid_preds_list.append(preds)\n",
    "    \n",
    "    \n",
    "#     model.eval()\n",
    "#     y_pred = model(torch.Tensor(df_test[fea_cols].values).to(device))    \n",
    "#     y_pred = np.stack(y_pred.cpu().detach().numpy())\n",
    "#     test_preds_list.append(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.539013Z",
     "start_time": "2020-02-01T13:11:38.532480Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train = pd.DataFrame(np.concatenate(train_preds_list, axis=1))\n",
    "\n",
    "# cols = []\n",
    "# for i in range(len(models)):\n",
    "#     cols.extend([f'pred_{i}_1', f'pred_{i}_2', f'pred_{i}_3', f'pred_{i}_4'])\n",
    "\n",
    "# df_train.columns = cols\n",
    "# df_train_y = pd.DataFrame(np.stack(train_y_true), columns = ['layer_1', 'layer_2', 'layer_3', 'layer_4'])\n",
    "\n",
    "# df_train = pd.concat([df_train, df_train_y], axis=1)\n",
    "\n",
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.544388Z",
     "start_time": "2020-02-01T13:11:38.540374Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_valid = pd.DataFrame(np.concatenate(valid_preds_list, axis=1))\n",
    "\n",
    "# cols = []\n",
    "# for i in range(len(models)):\n",
    "#     cols.extend([f'pred_{i}_1', f'pred_{i}_2', f'pred_{i}_3', f'pred_{i}_4'])\n",
    "\n",
    "# df_valid.columns = cols\n",
    "# df_valid_y = pd.DataFrame(np.stack(valid_y_true), columns = ['layer_1', 'layer_2', 'layer_3', 'layer_4'])\n",
    "\n",
    "# df_valid = pd.concat([df_valid, df_valid_y], axis=1)\n",
    "\n",
    "# df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.549928Z",
     "start_time": "2020-02-01T13:11:38.545372Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_score = pd.DataFrame(np.concatenate(test_preds_list, axis=1))\n",
    "\n",
    "# cols = []\n",
    "# for i in range(len(models)):\n",
    "#     cols.extend([f'pred_{i}_1', f'pred_{i}_2', f'pred_{i}_3', f'pred_{i}_4'])\n",
    "\n",
    "# df_score.columns = cols\n",
    "\n",
    "# df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.623326Z",
     "start_time": "2020-02-01T13:11:38.550834Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = joblib.load('df_pred_train_10.pkl')\n",
    "df_valid = joblib.load('df_pred_valid_10.pkl')\n",
    "df_score = joblib.load('df_pred_score_10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.290673Z",
     "start_time": "2020-02-01T13:06:52.289421Z"
    }
   },
   "outputs": [],
   "source": [
    "# joblib.dump(df_train, 'df_pred_train_10.pkl')\n",
    "# joblib.dump(df_valid, 'df_pred_valid_10.pkl')\n",
    "# joblib.dump(df_score, 'df_pred_score_10.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.296436Z",
     "start_time": "2020-02-01T13:06:52.291340Z"
    }
   },
   "outputs": [],
   "source": [
    "# fea_cols\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "\n",
    "# clf = Ridge(alpha=1.0)\n",
    "# clf.fit(df_train[fea_cols], df_train['layer_2'])\n",
    "\n",
    "# clf.predict(df_valid[fea_cols])\n",
    "\n",
    "# mean_absolute_error(df_valid['layer_2'], clf.predict(df_valid[fea_cols]))\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# reg = LinearRegression().fit(df_train[fea_cols], df_train['layer_2'])\n",
    "# reg.score(df_train[fea_cols], df_train['layer_2'])\n",
    "\n",
    "# reg.coef_\n",
    "\n",
    "# reg.intercept_\n",
    "\n",
    "# reg.predict(df_valid[fea_cols])\n",
    "\n",
    "# df_valid['layer_2']\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# mean_absolute_error(df_valid['layer_2'], reg.predict(df_valid[fea_cols]))\n",
    "\n",
    "# # y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "# # y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "# # mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# # mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "\n",
    "# # mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.304090Z",
     "start_time": "2020-02-01T13:06:52.297168Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(1, 5):\n",
    "#     layer_cols = [f'layer_{i}']\n",
    "#     fea_cols = [c for c in df_train.columns if c not in layer_cols and c[-1] == str(i)]\n",
    "\n",
    "#     print(fea_cols)\n",
    "#     print(layer_cols)\n",
    "\n",
    "#     model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "#     print('model_ts', model_ts)\n",
    "\n",
    "#     initscore_filename = ''\n",
    "#     params = {\n",
    "#         'boosting':'gbdt',\n",
    "#         'num_leaves': 7,\n",
    "#     #     'max_depth': 3,\n",
    "#         'objective': 'l2',\n",
    "#         'metric':'l1',\n",
    "#         'num_threads': 14,\n",
    "#         'learning_rate': 0.01,\n",
    "#         'device_type':'gpu',\n",
    "\n",
    "#     }\n",
    "#     print(params)\n",
    "\n",
    "#     data_params = {\n",
    "#     #     'max_bin':127,\n",
    "#     #     'enable_bundle': False,\n",
    "#     }\n",
    "#     print(data_params)\n",
    "\n",
    "#     num_round = 50000\n",
    "#     print('num_round:', num_round)\n",
    "\n",
    "#     train_set = lgb.Dataset(df_train[fea_cols].values, label=df_train[layer_cols[0]].values, params=data_params)\n",
    "#     val_set = lgb.Dataset(df_valid[fea_cols].values, label=df_valid[layer_cols[0]].values, params=data_params)\n",
    "\n",
    "#     evals_result = {}\n",
    "#     model = lgb.train(params, train_set, num_round, early_stopping_rounds=500, \n",
    "#                             valid_sets=[train_set, val_set],\n",
    "#                             verbose_eval=1000,\n",
    "#                             evals_result=evals_result,\n",
    "#                            )\n",
    "#     df_score[layer_cols[0]] = model.predict(df_score[fea_cols].values)\n",
    "#     print(df_score[fea_cols + layer_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.311729Z",
     "start_time": "2020-02-01T13:06:52.304743Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_score.index.name = 'id'\n",
    "# df_score[[c for c in df_score if 'layer' in c]]\n",
    "\n",
    "# for i in range(1,5):\n",
    "#     df_score[f'layer_{i}'] = df_score[[c for c in df_score.columns if c[0] == 'p' and c[-1] == str(i)]].mean(axis=1)\n",
    "\n",
    "# df_score\n",
    "\n",
    "# df_score[[c for c in df_score if 'layer' in c]]\n",
    "\n",
    "# df_score[[c for c in df_score if 'layer' in c]].to_csv('eeeee1.csv')\n",
    "\n",
    "# model.predict(df_score[fea_cols].values)\n",
    "\n",
    "# df_valid['layer_1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.382929Z",
     "start_time": "2020-02-01T13:06:52.312486Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, dropout_probability=0.3):\n",
    "        super(DNNModel,self).__init__()\n",
    "        act = torch.nn.ELU()\n",
    "        dropout = torch.nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        self.m1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128), torch.nn.BatchNorm1d(128), act, dropout, \n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout, \n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 1)\n",
    "        )\n",
    "        self.m2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128), torch.nn.BatchNorm1d(128), act, dropout, \n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout, \n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 1)\n",
    "        )\n",
    "        self.m3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128), torch.nn.BatchNorm1d(128), act, dropout, \n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout, \n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 1)\n",
    "        )\n",
    "        self.m4 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128), torch.nn.BatchNorm1d(128), act, dropout, \n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout, \n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x1 = x[:,0::4]\n",
    "        x2 = x[:,1::4]\n",
    "        x3 = x[:,2::4]\n",
    "        x4 = x[:,3::4]\n",
    "        return torch.cat([self.m1(x1), self.m2(x2), self.m3(x3), self.m4(x4)], axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.391843Z",
     "start_time": "2020-02-01T13:06:52.383753Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_cols = [c for c in df_train.columns if 'pred' in c]\n",
    "\n",
    "pred_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.396952Z",
     "start_time": "2020-02-01T13:06:52.392599Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(pd.concat([df_train, df_valid])[pred_cols].values)\n",
    "\n",
    "# df_train[pred_cols] = scaler.transform(df_train[pred_cols].values)\n",
    "# df_valid[pred_cols] = scaler.transform(df_valid[pred_cols].values)\n",
    "# df_score[pred_cols] = scaler.transform(df_score[pred_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.465117Z",
     "start_time": "2020-02-01T13:06:52.397727Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:52.468317Z",
     "start_time": "2020-02-01T13:06:52.465917Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# layer_cols = [c for c in df_train.columns if 'layer' in c]\n",
    "# fea_cols = pred_cols\n",
    "# print(fea_cols)\n",
    "# print(layer_cols)\n",
    "\n",
    "# train_set = SemiDataset(df_train, fea_cols, layer_cols)\n",
    "# val_set = SemiDataset(df_valid, fea_cols, layer_cols)\n",
    "\n",
    "# print(len(train_set), len(val_set))\n",
    "\n",
    "# #     batch_size = 700000 // 2\n",
    "# batch_size = 25000\n",
    "# num_workers = 8\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "# val_loader = DataLoader(dataset=val_set, batch_size=110000)\n",
    "\n",
    "# print(f'batch_size {batch_size} num_workers {num_workers}')\n",
    "# print(f'train_loader {len(train_loader)} val_loader {len(val_loader)}')\n",
    "\n",
    "# model = DNNModel(input_size=len(fea_cols) // 4, dropout_probability=0).to(device)\n",
    "\n",
    "# criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "# scheduler = StepLR(optimizer, step_size=50, gamma=1.0)\n",
    "# trainer = Trainer(model, criterion, optimizer, scheduler, device)\n",
    "\n",
    "\n",
    "# val_loss_min = np.Inf\n",
    "\n",
    "# lr_list = [\n",
    "#     (0.01, 20),\n",
    "#     (0.001, 20),\n",
    "#     (0.0001, 20),\n",
    "#     (0.00003, 20),\n",
    "#     (0.00001, 20),\n",
    "#     (0.000005, 20),\n",
    "# ]\n",
    "\n",
    "\n",
    "# total_epoch = 10000\n",
    "\n",
    "# for lr, patience in lr_list:\n",
    "#     print(lr, patience)\n",
    "#     if os.path.isfile('stop.flag'):\n",
    "#         print('stop!')\n",
    "#         break\n",
    "\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_epoch=1, verbose=True)\n",
    "#     early_stopping.val_loss_min = val_loss_min\n",
    "#     early_stopping.best_score = None if val_loss_min==np.Inf else -val_loss_min \n",
    "\n",
    "# #     criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "\n",
    "#     trainer.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# #     trainer.optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# #     trainer.optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "#     trainer.scheduler = StepLR(trainer.optimizer, step_size=50, gamma=1.0)\n",
    "\n",
    "#     for e in tqdm_notebook(range(total_epoch), total=total_epoch, desc='Epoch'):\n",
    "#         if os.path.isfile('stop.flag'):\n",
    "#             print(f'{e} stop!')\n",
    "#             break\n",
    "\n",
    "#         train_loss = trainer.train(train_loader)\n",
    "\n",
    "#         if e % 1 == 0:\n",
    "#             valid_loss = trainer.eval(val_loader)\n",
    "#     #         valid_loss = train_loss\n",
    "\n",
    "#             ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "#             print(f'[{ts}] Epock {e} / {total_epoch}\\t lr {trainer.scheduler.get_lr()[0]}')\n",
    "#             print(f'  train_loss: {train_loss}  valid_loss: {valid_loss}')\n",
    "\n",
    "#             early_stopping(valid_loss, model)\n",
    "\n",
    "#             if early_stopping.early_stop:\n",
    "#                 print(\"\\tEarly stopping epoch {}, valid loss {}\".format(e, early_stopping.val_loss_min))\n",
    "#                 break\n",
    "\n",
    "#     model.load_state_dict(torch.load('model/checkpoint.pt'))\n",
    "#     val_loss_min = early_stopping.val_loss_min\n",
    "\n",
    "\n",
    "#     model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "#     print(model_ts)\n",
    "#     model_path = 'model/e{}_{}'.format(model_ts, val_loss_min)\n",
    "#     trainer.save('{}.model'.format(model_path))\n",
    "#     print(model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:38.951461Z",
     "start_time": "2020-02-01T13:11:38.946544Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, dropout_probability=0.3, dims=32):\n",
    "        super(DNNModel,self).__init__()\n",
    "        act = torch.nn.ELU()\n",
    "        dropout = torch.nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        self.m1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, dims), torch.nn.BatchNorm1d(dims), act, dropout, \n",
    "            torch.nn.Linear(dims, dims), torch.nn.BatchNorm1d(dims), act, dropout, \n",
    "            torch.nn.Linear(dims, dims), torch.nn.BatchNorm1d(dims), act, dropout,\n",
    "            torch.nn.Linear(dims, dims), torch.nn.BatchNorm1d(dims), act, dropout,\n",
    "            torch.nn.Linear(dims, dims), torch.nn.BatchNorm1d(dims), act, dropout,\n",
    "            torch.nn.Linear(dims, dims), torch.nn.BatchNorm1d(dims), act, dropout,\n",
    "#             torch.nn.Linear(128, 256), torch.nn.BatchNorm1d(256), act, dropout,\n",
    "#             torch.nn.Linear(256, 256), torch.nn.BatchNorm1d(256), act, dropout,\n",
    "#             torch.nn.Linear(256, 256), torch.nn.BatchNorm1d(256), act, dropout,\n",
    "#             torch.nn.Linear(256, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "#             torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "#             torch.nn.Linear(128, 128), torch.nn.BatchNorm1d(128), act, dropout,\n",
    "            torch.nn.Linear(dims, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.m1(x)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:11:39.632308Z",
     "start_time": "2020-02-01T13:11:39.630500Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_batch = []\n",
    "# y_batch = []\n",
    "# for i in [1, 2, 3, 4]:\n",
    "#     layer_cols = [f'layer_{i}']\n",
    "#     fea_cols = [c for c in df_train.columns if c not in layer_cols and c[-1] == str(i)]\n",
    "#     X_batch.append(torch.Tensor(df_train[fea_cols].values).to(device))\n",
    "#     y_batch.append(torch.Tensor(df_train[layer_cols].values).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T00:58:22.692991Z",
     "start_time": "2020-02-01T13:12:55.589890Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dims in range(192, 550, 32):\n",
    "    print('dims', dims)\n",
    "    model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "    for i in [1, 2, 3, 4]:\n",
    "        layer_cols = [f'layer_{i}']\n",
    "        fea_cols = [c for c in df_train.columns if c not in layer_cols and c[-1] == str(i)]\n",
    "        print(fea_cols)\n",
    "        print(layer_cols)\n",
    "\n",
    "        train_set = SemiDataset(df_train, fea_cols, layer_cols)\n",
    "        val_set = SemiDataset(df_valid, fea_cols, layer_cols)\n",
    "\n",
    "        print(len(train_set), len(val_set))\n",
    "\n",
    "#         batch_size = 700000 // 2\n",
    "        batch_size = 70000\n",
    "        num_workers = 8\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "        val_loader = DataLoader(dataset=val_set, batch_size=110000)\n",
    "\n",
    "        print(f'batch_size {batch_size} num_workers {num_workers}')\n",
    "        print(f'train_loader {len(train_loader)} val_loader {len(val_loader)}')\n",
    "\n",
    "        model = DNNModel(input_size=len(fea_cols), dropout_probability=0, dims=dims).to(device)\n",
    "\n",
    "        criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "        scheduler = StepLR(optimizer, step_size=50, gamma=1.0)\n",
    "        trainer = Trainer(model, criterion, optimizer, scheduler, device)\n",
    "\n",
    "\n",
    "        val_loss_min = np.Inf\n",
    "\n",
    "        lr_list = [\n",
    "            (0.01, 20),\n",
    "            (0.001, 20),\n",
    "            (0.0001, 20),\n",
    "            (0.00003, 20),\n",
    "            (0.00001, 20),\n",
    "            (0.000005, 20),\n",
    "        ]\n",
    "\n",
    "\n",
    "        total_epoch = 10000\n",
    "\n",
    "        for lr, patience in lr_list:\n",
    "            print(lr, patience)\n",
    "            if os.path.isfile('stop.flag'):\n",
    "                print('stop!')\n",
    "                break\n",
    "\n",
    "            early_stopping = EarlyStopping(patience=patience, min_epoch=1, verbose=True)\n",
    "            early_stopping.val_loss_min = val_loss_min\n",
    "            early_stopping.best_score = None if val_loss_min==np.Inf else -val_loss_min \n",
    "\n",
    "        #     criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "\n",
    "            trainer.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        #     trainer.optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        #     trainer.optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "            trainer.scheduler = StepLR(trainer.optimizer, step_size=50, gamma=1.0)\n",
    "\n",
    "            for e in tqdm_notebook(range(total_epoch), total=total_epoch, desc='Epoch'):\n",
    "                if os.path.isfile('stop.flag'):\n",
    "                    print(f'{e} stop!')\n",
    "                    break\n",
    "\n",
    "                train_loss = trainer.train(train_loader)\n",
    "#                 train_loss = trainer.train_(X_batch[i], y_batch[i])\n",
    "\n",
    "                if e % 1 == 0:\n",
    "                    valid_loss = trainer.eval(val_loader)\n",
    "            #         valid_loss = train_loss\n",
    "\n",
    "                    ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "                    print(f'[{ts}] Epock {e} / {total_epoch}\\t lr {trainer.scheduler.get_lr()[0]}')\n",
    "                    print(f'  train_loss: {train_loss}  valid_loss: {valid_loss}')\n",
    "\n",
    "                    early_stopping(valid_loss, model)\n",
    "\n",
    "                    if early_stopping.early_stop:\n",
    "                        print(\"\\tEarly stopping epoch {}, valid loss {}\".format(e, early_stopping.val_loss_min))\n",
    "                        break\n",
    "\n",
    "            model.load_state_dict(torch.load('model/checkpoint.pt'))\n",
    "            val_loss_min = early_stopping.val_loss_min\n",
    "\n",
    "\n",
    "\n",
    "            print(model_ts)\n",
    "            model_path = 'model/{}_e{}_{}_{}'.format(model_ts, dims, i, val_loss_min)\n",
    "            trainer.save('{}.model'.format(model_path))\n",
    "            print(model_path)\n",
    "\n",
    "    #     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:53.372776Z",
     "start_time": "2020-02-01T13:05:30.578Z"
    }
   },
   "outputs": [],
   "source": [
    "# e_models =[\n",
    "#     'model/20200131T225354_e1_0.32568883895874023.model',\n",
    "#     'model/20200131T225354_e2_0.5253744721412659.model',\n",
    "#     'model/20200131T225354_e3_0.3913254737854004.model',\n",
    "#     'model/20200131T225354_e4_0.3752939999103546.model',\n",
    "# ]\n",
    "\n",
    "# for i in range(1, 5):\n",
    "#     layer_cols = [f'layer_{i}']\n",
    "#     fea_cols = [c for c in df_score.columns if c not in layer_cols and c[-1] == str(i)]\n",
    "#     print(fea_cols)\n",
    "#     print(layer_cols)\n",
    "#     y_pred = joblib.load(e_models[i-1])(torch.Tensor(df_score[fea_cols].values).to(device))    \n",
    "#     df_score[layer_cols[0]] = y_pred.cpu().detach().numpy()\n",
    "#     print(df_score[layer_cols + fea_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:53.373216Z",
     "start_time": "2020-02-01T13:05:30.581Z"
    }
   },
   "outputs": [],
   "source": [
    "# e_models = [\n",
    "#     'model/e_1_20200131T202847_0.322307288646698.model',\n",
    "# #     'model/e_1_20200131T221837_0.33561456203460693.model',\n",
    "#     'model/e_2_20200131T213251_0.5193350911140442.model',\n",
    "#     'model/e_3_20200131T214620_0.38977283239364624.model',\n",
    "#     'model/e_4_20200131T211402_0.37712812423706055.model',\n",
    "# #     'model/e_4_20200131T220102_0.3780273497104645.model',\n",
    "# #     'model/e_3_20200131T214620_0.38977283239364624.model',\n",
    "# #     'model/e_4_20200131T211402_0.37712812423706055.model',\n",
    "# #     'model/e_3_20200131T205640_0.3976164758205414.model',\n",
    "# # #     'model/e_2_20200131T204154_0.5304651260375977.model',\n",
    "# #     'model/e_1_20200131T202847_0.322307288646698.model',\n",
    "# ]\n",
    "# for i in range(1, 5):\n",
    "#     layer_cols = [f'layer_{i}']\n",
    "#     fea_cols = [c for c in df_score.columns if c not in layer_cols and c[-1] == str(i)]\n",
    "#     print(fea_cols)\n",
    "#     print(layer_cols)\n",
    "#     y_pred = joblib.load(e_models[i-1])(torch.Tensor(df_score[fea_cols].values).to(device))    \n",
    "#     df_score[layer_cols[0]] = y_pred.cpu().detach().numpy()\n",
    "#     print(df_score[layer_cols + fea_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:53.373650Z",
     "start_time": "2020-02-01T13:05:30.584Z"
    }
   },
   "outputs": [],
   "source": [
    "df_score.index.name = 'id'\n",
    "layer_cols = [c for c in df_score.columns if 'la' in c]\n",
    "ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "df_score[layer_cols].to_csv(f'submit/{ts}_e.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:06:53.374086Z",
     "start_time": "2020-02-01T13:05:30.588Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #### Pred Train\n",
    "\n",
    "# criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "# model.train()\n",
    "# total_loss = 0\n",
    "# preds = []\n",
    "# y_true = []\n",
    "# for data in train_loader:\n",
    "#     X_batch, y_batch = data\n",
    "#     X_batch = X_batch.to(device)\n",
    "#     y_batch = y_batch.to(device)\n",
    "#     y_true.extend(y_batch)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         y_pred = model(X_batch)\n",
    "#         preds.extend(y_pred)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         total_loss = total_loss + loss.item()\n",
    "        \n",
    "# train_loss_min = total_loss / len(train_loader)\n",
    "# train_loss_min\n",
    "\n",
    "# y_true\n",
    "\n",
    "# preds\n",
    "\n",
    "# #### Pred Valid\n",
    "\n",
    "# criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "# model.eval()\n",
    "# total_loss = 0\n",
    "# for data in val_loader:\n",
    "#     X_batch, y_batch = data\n",
    "#     X_batch = X_batch.to(device)\n",
    "#     y_batch = y_batch.to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         y_pred = model(X_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         total_loss = total_loss + loss.item()\n",
    "        \n",
    "# val_loss_min = total_loss / len(val_loader)\n",
    "# val_loss_min\n",
    "\n",
    "# #### Pred Test \n",
    "\n",
    "# model.eval()\n",
    "# y_pred = model(torch.Tensor(df_test[fea_cols].values).to(device))    \n",
    "# print(y_pred)\n",
    "\n",
    "# ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "# df_submit = pd.read_csv('input/sample_submission.csv', index_col=0)\n",
    "\n",
    "# df_submit[layer_cols] = y_pred.cpu().detach().numpy()\n",
    "# df_submit.to_csv(f'submit/{ts}_{val_loss_min}.csv')\n",
    "\n",
    "# print(ts, val_loss_min)\n",
    "\n",
    "# # model.load_state_dict(torch.load('model/20200123T215856_0.6811393996079763.model'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "166px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
